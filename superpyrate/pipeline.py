"""
bash to prepare some ais files for testing:

>>> for f in aiscsv/*.csv ; do head -n 100 "$f" >> $(echo $f | sed s/aiscsv/testais/); done

"""
import luigi
from luigi import six, postgres
from pyrate.algorithms.aisparser import readcsv, parse_raw_row, AIS_CSV_COLUMNS, validate_row
from pyrate.repositories.aisdb import AISdb
import csv
import datetime
import psycopg2
import logging
import tempfile
logger = logging.getLogger('luigi-interface')


class Pipeline(luigi.WrapperTask):
    """Wrapper task which performs the entire pipeline
    """

    date_interval = luigi.DateIntervalParameter()
    def requires(self):
        yield [ValidMessagesToDatabase(date) for date in self.date_interval]


class SourceFiles(luigi.ExternalTask):
    """
    exactEarth_historical_data_2013-07-15_1.csv

    """
    date = luigi.DateParameter()
    def output(self):
        return luigi.file.LocalTarget(self.date.strftime('./aiscsv/exactEarth_historical_data_%Y%m%d.csv'))


class ValidMessages(luigi.Task):
    """ Takes AIS messages and runs validation functions, generating valid csv
    files
    """

    date = luigi.DateParameter()
    def requires(self):
        return SourceFiles(self.date)

    def run(self):
        iterator = readcsv(self.input().open('r'))
        # Do validation and write a new file of valid messages
        f = self.output().open('w')
        writer = csv.DictWriter(f, dialect="excel", fieldnames=AIS_CSV_COLUMNS)
        writer.writeheader()

        # parse and iterate lines from the current file
        for row in iterator:
            converted_row = {}
            try:
                # parse raw data
                converted_row = parse_raw_row(row)
            except ValueError as e:
                # invalid data in row. Write it to error log
                if not 'raw' in row:
                    row['raw'] = [row[c] for c in AIS_CSV_COLUMNS]
                continue
            except KeyError:
                # missing data in row.
                if not 'raw' in row:
                    row['raw'] = [row[c] for c in AIS_CSV_COLUMNS]
                continue

            # validate parsed row and add to appropriate queue
            try:
                validated_row = validate_row(converted_row)
                writer.writerow(validated_row)
            except ValueError:
                pass
        f.close()

    def output(self):
        return luigi.file.LocalTarget(self.date.strftime('./validcsv/exactEarth_historical_data_%Y%m%d.csv'))


class ValidMessagesToDatabase(luigi.postgres.CopyToTable):
    date = luigi.DateParameter()

    null_values = (None,"")
    column_separator = ","

    host = "localhost"
    database = "test_aisdb"
    user = "test_ais"
    password = ""
    table = "ais_clean"

    # aisdb = AISdb({'host': host,
    #                'db' : database,
    #                'user' : user,
    #                'pass' : password})
    # lc_cols = aisdb.clean_db_spec['cols']

    cols = ['MMSI','Time','Message_ID','Navigational_status','SOG',
               'Longitude','Latitude','COG','Heading','IMO','Draught',
               'Destination','Vessel_Name',
               'ETA_month','ETA_day','ETA_hour','ETA_minute']
    columns = [x.lower() for x in cols]
    # logger.debug("Columns: {}".format(columns))

    def rows(self):
        """
        Return/yield tuples or lists corresponding to each row to be inserted.
        """
        with self.input().open('r') as csvfile:
            reader = csv.reader(csvfile)
            for row in reader:
                yield row
                # logger.debug(line)
                # yield [x for x in line.strip('\n').split(',') ]

    def requires(self):
        return ValidMessages(self.date)

    def copy(self, cursor, file):
        if isinstance(self.columns[0], six.string_types):
            column_names = self.columns
        elif len(self.columns[0]) == 2:
            column_names = [c[0] for c in self.columns]
        else:
            raise Exception('columns must consist of column strings or (column string, type string) tuples (was %r ...)' % (self.columns[0],))
        logger.debug(self.columns)
        sql = "COPY {} ({}) FROM STDIN WITH (FORMAT csv, HEADER true)".format(self.table, ",".join(self.columns), file)
        # self.table, self.columns, file,
        # cursor.copy_from(file,
        #                  self.table,
        #                  null=r"",
        #                  sep=self.column_separator,
        #                  columns=self.columns)
        cursor.copy_expert(sql, file)

    def run(self):
        """
        Inserts data generated by rows() into target table.

        If the target table doesn't exist, self.create_table will be called to attempt to create the table.

        Normally you don't want to override this.
        """
        if not (self.table and self.columns):
            raise Exception("table and columns need to be specified")

        connection = self.output().connect()

        with self.input().open('r') as csvfile:
            for attempt in range(2):
                try:
                    cursor = connection.cursor()
                    # self.init_copy(connection)
                    self.copy(cursor, csvfile)
                    # self.post_copy(connection)
                except psycopg2.ProgrammingError as e:
                    if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:
                        # if first attempt fails with "relation not found", try creating table
                        logger.info("Creating table %s", self.table)
                        connection.reset()
                        self.create_table(connection)
                    else:
                        raise
                else:
                    break

        # # transform all data generated by rows() using map_column and write data
        # # to a temporary file for import using postgres COPY
        # tmp_dir = luigi.configuration.get_config().get('postgres', 'local-tmp-dir', None)
        # tmp_file = tempfile.TemporaryFile(dir=tmp_dir)
        # n = 0
        # for row in self.rows():
        #     n += 1
        #     if n % 100000 == 0:
        #         logger.info("Wrote %d lines", n)
        #     rowstr = self.column_separator.join(self.map_column(val) for val in row)
        #     logger.debug(rowstr)
        #     rowstr += "\n"
        #     tmp_file.write(rowstr.encode('utf-8'))
        #
        # logger.info("Done writing, importing at %s", datetime.datetime.now())
        # tmp_file.seek(0)
        #
        # # attempt to copy the data into postgres
        # # if it fails because the target table doesn't exist
        # # try to create it by running self.create_table
        # for attempt in range(2):
        #     try:
        #         cursor = connection.cursor()
        #         self.init_copy(connection)
        #         self.copy(cursor, tmp_file)
        #         self.post_copy(connection)
        #     except psycopg2.ProgrammingError as e:
        #         if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:
        #             # if first attempt fails with "relation not found", try creating table
        #             logger.info("Creating table %s", self.table)
        #             connection.reset()
        #             self.create_table(connection)
        #         else:
        #             raise
        #     else:
        #         break

        # mark as complete in same transaction
        self.output().touch(connection)

        # commit and clean up
        connection.commit()
        connection.close()
        # tmp_file.close()
